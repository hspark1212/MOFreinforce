{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import BRICS\n",
    "from rdkit import RDLogger\n",
    "import libs.selfies as sf\n",
    "\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing - PORMAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_to_idx = json.load(open(\"data/mc_to_idx.json\"))\n",
    "topo_to_idx = json.load(open(\"data/topo_to_idx.json\"))\n",
    "len(mc_to_idx), len(topo_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_to_smiles = json.load(open(\"data/ol_to_smiles.json\"))\n",
    "ol_to_selfies = json.load(open(\"data/ol_to_selfies.json\"))\n",
    "print(len(ol_to_smiles), len(ol_to_selfies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset for Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) BRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ol_input(smile):\n",
    "    pairs = []\n",
    "    # find the number of connection points\n",
    "    num_conn = len(re.findall(\"\\*\", smile))\n",
    "\n",
    "    # BRICS composition\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    frags = BRICS.BRICSDecompose(mol)\n",
    "    frags = np.array(sorted({re.sub(r\"\\[\\d+\\*\\]\", \"*\", f) for f in frags}))\n",
    "\n",
    "    # replace * with H\n",
    "    du, hy = Chem.MolFromSmiles(\"*\"), Chem.MolFromSmiles(\"[H]\")\n",
    "    subs = np.array([Chem.MolFromSmiles(f) for f in frags])\n",
    "    subs = np.array(\n",
    "        [\n",
    "            Chem.RemoveHs(Chem.ReplaceSubstructs(f, du, hy, replaceAll=True)[0])\n",
    "            for f in subs\n",
    "        ]\n",
    "    )\n",
    "    subs = np.array([m for m in subs if m.GetNumAtoms() > 1])\n",
    "\n",
    "    # delete substruct of frags\n",
    "    match = np.array([[m.HasSubstructMatch(f) for f in subs] for m in subs])\n",
    "    frags = subs[match.sum(axis=0) == 1]\n",
    "\n",
    "    # only use top four frags with descending order of number of atoms\n",
    "    frags = sorted(frags, key=lambda x: -x.GetNumAtoms())[:4]  # [:voc.n_frags]\n",
    "    frags = [Chem.MolToSmiles(Chem.RemoveHs(f)) for f in frags]\n",
    "\n",
    "    # encoding selfies\n",
    "    smile_sf = sf.encoder(smile)\n",
    "    frags = [sf.encoder(f) for f in frags]\n",
    "\n",
    "    max_comb = len(frags)\n",
    "    for ix in range(1, max_comb + 1):\n",
    "        combs = combinations(frags, ix)\n",
    "        for comb in combs:\n",
    "\n",
    "            comb_frags = \".\".join(comb)\n",
    "\n",
    "            if (\n",
    "                len(comb_frags) > len(smile_sf)\n",
    "                or len(list(sf.split_selfies(smile_sf))) > 98\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # if mol.HasSubstructMatch(Chem.MolFromSmarts(input)):\n",
    "            pairs.append([num_conn, comb_frags, smile_sf])\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cif_ids = json.load(open(\"assets/success_cif_ids.json\"))  # rmsd < 0.25\n",
    "len(cif_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_mof = []\n",
    "for mof_name, _ in tqdm(cif_ids.items()):\n",
    "    topo, mc, ol = mof_name.split(\"+\")\n",
    "\n",
    "    if ol not in ol_to_selfies:\n",
    "        continue\n",
    "    ol_sm = ol_to_smiles[ol]\n",
    "    pairs_ol = make_ol_input(ol_sm)\n",
    "\n",
    "    for pair_ol in pairs_ol:\n",
    "        pairs_mof.append([topo, mc, ol] + pair_ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    pairs_mof, columns=[\"topo\", \"mc\", \"ol\", \"num_conn\", \"frags\", \"selfies\"]\n",
    ")\n",
    "# df.to_csv(\"data/dataset_generator/raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/dataset_generator/raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for i in tqdm(range(len(df))):\n",
    "    data = df.iloc[i]\n",
    "    frags = data[\"frags\"]\n",
    "    selfies = data[\"selfies\"]\n",
    "    counter.update(sf.split_selfies(frags))\n",
    "    counter.update(sf.split_selfies(selfies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(counter.keys())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_idx = {}\n",
    "vocab_to_idx[\"[PAD]\"] = 0\n",
    "vocab_to_idx[\"[SOS]\"] = 1\n",
    "vocab_to_idx[\"[EOS]\"] = 2\n",
    "vocab_to_idx.update({s: i + 3 for i, s in enumerate(vocab)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(vocab_to_idx, open(\"data/vocab_to_idx.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_idx = json.load(open(\"data/vocab_to_idx.json\"))\n",
    "len(vocab_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Split train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(df) * 0.8)\n",
    "k_ = int(len(df) * 0.9)\n",
    "# k = int(1e6)\n",
    "# k_ = int(1e5)\n",
    "\n",
    "train = df.iloc[:k]\n",
    "val = df.iloc[k : k + k_]\n",
    "test = df.iloc[-int(1e4) :]\n",
    "print(len(train), len(val), len(test))\n",
    "train.to_csv(\"data/dataset_generator/train.csv\")\n",
    "val.to_csv(\"data/dataset_generator/val.csv\")\n",
    "test.to_csv(\"data/dataset_generator/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_idx = json.load(open(\"data/vocab_to_idx.json\"))\n",
    "train = pd.read_csv(\"data/dataset_generator/train.csv\")\n",
    "val = pd.read_csv(\"data/dataset_generator/val.csv\")\n",
    "test = pd.read_csv(\"data/dataset_generator/test.csv\")\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags = np.array(train[\"frags\"])\n",
    "num_conn = np.array(train[\"num_conn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for f in tqdm(frags):\n",
    "    counter.update(f.split(\".\"))\n",
    "vocab_frag = list(counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = copy.deepcopy(train[:100000])\n",
    "new_val = copy.deepcopy(val[:10000])\n",
    "new_test = copy.deepcopy(test)\n",
    "split = [new_train, new_val, new_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in split:\n",
    "    num = len(s) / 2\n",
    "    for i in tqdm(range(int(num))):\n",
    "\n",
    "        num_frags = np.random.choice(range(1, 5), 1, p=[0.3, 0.3, 0.3, 0.1])[0]\n",
    "\n",
    "        check = True\n",
    "        while check:\n",
    "            new_frags = \".\".join(np.random.choice(vocab_frag, num_frags))\n",
    "            if len(list(sf.split_selfies(new_frags))) < 100:\n",
    "                check = False\n",
    "\n",
    "        s[\"frags\"].iloc[i] = new_frags\n",
    "    s.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"train\", \"val\", \"test\"]\n",
    "for s in split:\n",
    "    s.to_csv(f\"data/dataset_reinforce/{s}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv(\"data/dataset_reinforce/train.csv\")\n",
    "new_val.to_csv(\"data/dataset_reinforce/val.csv\")\n",
    "new_test.to_csv(\"data/dataset_reinforce/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset for Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_to_idx = json.load(open(\"data/mc_to_idx.json\"))\n",
    "topo_to_idx = json.load(open(\"data/topo_to_idx.json\"))\n",
    "ol_to_smiles = json.load(open(\"data/ol_to_smiles.json\"))\n",
    "ol_to_selfies = json.load(open(\"data/ol_to_selfies.json\"))\n",
    "len(mc_to_idx), len(topo_to_idx), len(ol_to_smiles), len(ol_to_selfies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_idx = json.load(open(\"data/vocab_to_idx.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mof_to_rmsd = json.load(open(\"assets/mof_to_rmsd.json\"))\n",
    "vocab_to_idx = json.load(open(\"data/vocab_to_idx.json\"))\n",
    "len(mof_to_rmsd), len(vocab_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mof = {}\n",
    "for mof_name, target in mof_to_rmsd.items():\n",
    "    mof = {}\n",
    "    topo, mc, ol = mof_name.split(\"+\")\n",
    "\n",
    "    if ol not in ol_to_selfies:\n",
    "        continue\n",
    "\n",
    "    mof[\"topo_name\"] = topo\n",
    "    mof[\"mc_name\"] = mc\n",
    "    mof[\"ol_name\"] = ol\n",
    "\n",
    "    # get smiles of oragnic linker (add start and end token)\n",
    "    # mof[\"ol_smiles\"] = \"<\" + ol_to_smiles[ol] + \">\"\n",
    "    mof[\"ol_selfies\"] = \"[SOS]\" + ol_to_selfies[ol] + \"[EOS]\"\n",
    "    # make sequence of MOF\n",
    "    mof[\"topo\"] = topo_to_idx[topo]\n",
    "    mof[\"mc\"] = mc_to_idx[mc]\n",
    "\n",
    "    ol = sf.selfies_to_encoding(\n",
    "        selfies=ol_to_selfies[ol], vocab_stoi=vocab_to_idx, enc_type=\"label\"\n",
    "    )\n",
    "    mof[\"ol\"] = [1] + ol + [2]\n",
    "    # get target\n",
    "    dict_mof[mof_name] = mof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mof = json.load(open(\"data/dict_mof.json\"))\n",
    "len(dict_mof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Q_kh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### charge check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_cif = glob.glob(\"assets/co2/uff_eqeq_cifs/charge_cifs_batch*/*.cif\")\n",
    "len(filenames_cif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cif_ids = []\n",
    "fail_cif_ids = []\n",
    "for filename in tqdm(filenames_cif):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        f.close()\n",
    "\n",
    "    fail = False\n",
    "    for line in lines[16:]:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) > 5:\n",
    "            c = float(tokens[-1])\n",
    "\n",
    "            if not -3 < c < 3:\n",
    "                fail = True\n",
    "                break\n",
    "    cif_id = filename.split(\"/\")[-1][:-4].split(\"_charge\")[0]\n",
    "    if fail is False:\n",
    "        target_cif_ids.append(cif_id)\n",
    "    else:\n",
    "        fail_cif_ids.append(cif_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_cif_ids), len(fail_cif_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cif_ids = []\n",
    "fail_cif_ids = []\n",
    "for filename in tqdm(filenames_cif):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        f.close()\n",
    "\n",
    "    fail = False\n",
    "    for line in lines[16:]:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) > 5:\n",
    "            c = float(tokens[-1])\n",
    "\n",
    "            if not -3 < c < 3:\n",
    "                fail = True\n",
    "                break\n",
    "    cif_id = filename.split(\"/\")[-1][:-4].split(\"_charge\")[0]\n",
    "    if fail is False:\n",
    "        target_cif_ids.append(cif_id)\n",
    "    else:\n",
    "        fail_cif_ids.append(cif_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mof_to_qkh = {}\n",
    "for csv_file in tqdm(glob.glob(\"assets/co2/*.csv\")):\n",
    "    csv_ = pd.read_csv(csv_file)\n",
    "\n",
    "    for i in range(len(csv_)):\n",
    "        data = csv_.iloc[i]\n",
    "        cif_id = data[\"structure\"].split(\".\")[0].split(\"_charge\")[0]\n",
    "\n",
    "        if cif_id not in target_cif_ids:\n",
    "            continue\n",
    "\n",
    "        qkh = float(data[\"q_kh_co2\"])\n",
    "        if -100 < qkh < 0:\n",
    "            mof_to_qkh[cif_id] = float(data[\"q_kh_co2\"])\n",
    "\n",
    "len(mof_to_qkh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mof_names = list(set(mof_to_qkh.keys()) & set(dict_mof.keys()))\n",
    "len(mof_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_qkh = {}\n",
    "for mof_name in tqdm(mof_names):\n",
    "    # get target\n",
    "\n",
    "    d_ = dict_mof[mof_name]\n",
    "    d_.update({\"target\": mof_to_qkh[mof_name]})\n",
    "    dict_qkh[mof_name] = d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cif_id = list(dict_qkh.keys())\n",
    "random.shuffle(cif_id)\n",
    "k = int(len(cif_id) * 0.8)\n",
    "k_ = int(len(cif_id) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cif_id[:k]\n",
    "test = cif_id[k:k_]\n",
    "val = cif_id[k_:]\n",
    "print(len(train), len(test), len(val))\n",
    "split = [\"train\", \"test\", \"val\"]\n",
    "data = [train, test, val]\n",
    "for i, s in enumerate(split):\n",
    "    print(s)\n",
    "    d = {n: dict_qkh[n] for n in data[i]}\n",
    "    print(len(d))\n",
    "    json.dump(d, open(f\"data/dataset_predictor/qkh/{s}.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOFreinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
